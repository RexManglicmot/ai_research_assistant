# Status

![status](https://img.shields.io/badge/status-actively--developed-yellowgreen)

This project is **currently** being developed and improved with additional features and testing.

## Inspiration

During my undergrad and grad studies, we were given enormous stacks on stacks of papers to read. A bit too much! Hours were spent reading to prepare for lecture. A fellow grad student of mine told me that is impossible to read everything and recommended to skim for main points, which I ultimately did. Although this skill was useful back then, I wish there was a tool that would allow synthesize main findings. Thus, the inspiration for this particular project. 

# Introduction

This project is meant to be a **production-style code** Retrieval-Augmented Generation (RAG) model that lets users query one or more research papers in natural language. The assistant uses a free Hugging Face-hosted large language model (mistralai/Mistral-7B-Instruct-v0.2) and FAISS-powered semantic search to generate answers grounded in the content of uploaded PDFs.

## Metrics

This project will focus on the 4 generation metrics proposed by[Langchain](https://docs.smith.langchain.com/evaluation/tutorials/rag) and using LLM as a Judge concept that assess and score outputs like answers and summaries against specific guidelines. It acts as an automated evaluator that would otherwise require human review. The gold standard is to have a human evaluator score as well, and compare and contrast the scores of the Judge and Human, but that is outside this project scope and could be a potential follow up project.

1. **Correctness** â€“ Accuracy vs. a gold/reference answer. *(Requires reference)*
2. **Relevance** â€“ How well the answer addresses the question.
3. **Groundedness** â€“ Whether all claims are supported by retrieved context (no hallucinations).
4. **Retrieval Relevance** â€“ How relevant retrieved chunks are to the question.

**Scoring Rubric:**

| Score | Meaning |
|-------|---------|
| **5** | Perfect â€“ fully correct/relevant/grounded |
| **4** | Good â€“ minor issues only |
| **3** | Fair â€“ noticeable gaps or noise |
| **2** | Poor â€“ significant errors or irrelevance |
| **1** | Very poor â€“ mostly wrong/off-topic |
| **0** | Invalid â€“ empty, refusal, or broken output |

**Overall Score Calculation:**

\[
\text{Overall} = 0.35 \cdot \text{Correctness} + 0.25 \cdot \text{Groundedness} + 0.20 \cdot \text{Relevance} + 0.20 \cdot \text{Retrieval Relevance}
\]

**Passing Targets:**
- Correctness â‰¥ **4.0**
- Groundedness â‰¥ **4.5**
- Relevance â‰¥ **4.0**
- Retrieval Relevance â‰¥ **4.0**

## Why This Project Matters

Help students get relevant info and be more efficient with their time.

## Tech Stack

Python Â· sentence-transformers Â· FAISS Â· transformers Â· Mistral-7B-Instruct (Hugging Face) Â· cosine similarity Â· PyMuPDF Â· Tesseract OCR Â· Streamlit Â· pytest Â· logging Â· GitHub Actions Â· Docker
## Project Architecture

`
## ðŸ“Œ Project Flow

User Query
â”‚
â”œâ”€â”€ ingestion.py â†’ Load PDFs (optional OCR) â†’ extract text
â”‚
â”œâ”€â”€ utils.py â†’ chunk text + metadata (source, page)
â”‚
â”œâ”€â”€ embeddings.py â†’ create embeddings â†’ build/load FAISS index
â”‚
â”œâ”€â”€ agent.py â†’ retrieve top-K chunks via cosine similarity
â”‚
â”œâ”€â”€ agent.py â†’ send query + context to HF LLM (Mistral-7B-Instruct)
â”‚
â”œâ”€â”€ logger_config.py â†’ log query, retrieved docs, and output
â”‚
â”œâ”€â”€ Return â†’ Answer + citations
â”‚
â””â”€â”€ evaluation/ â†’ LLM-as-judge scores:
       â€¢ Correctness
       â€¢ Relevance
       â€¢ Groundedness
       â€¢ Retrieval Relevance
`

## Setup
TBD

## Limitations
This project works well for textual data and not optimized for images and tabular forms of data. Thus, Multimodality is another potential follow-up project. 